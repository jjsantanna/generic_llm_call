{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98bf6da9-7e45-4795-bee7-dd4172091898",
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_ENDPOINT = \"\"\n",
    "AZURE_OPENAI_KEY = \"\"\n",
    "AZURE_OPENAI_VERSION = \"\"\n",
    "\n",
    "# https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models\n",
    "AZURE_OPENAI_MODEL_35_0301 = \"gpt35turbo_v0301\" #it is actually 1106 (has json)\n",
    "AZURE_OPENAI_MODEL_4_1106 = \"gpt4_v1106\" #it is actually 1106"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731f5763-3311-45db-884b-67b63ff45ddc",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7985d824-65af-4123-8df4-d39dd4d5b53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install ollama\n",
    "from openai import AzureOpenAI\n",
    "import time\n",
    "import json\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd9b36-3391-4008-9190-a84c380c577a",
   "metadata": {},
   "source": [
    "## GPT 3.5 Turbo (version 1106 with json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f42ad23d-277f-478f-aa71-b9bf9bfc540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt35turbo_v1106(key, system_prompt, user_prompt):\n",
    "    try:\n",
    "        client = AzureOpenAI(azure_endpoint=AZURE_OPENAI_ENDPOINT, \n",
    "                             api_key=key, \n",
    "                             api_version=AZURE_OPENAI_VERSION)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_MODEL_35_0301,\n",
    "            messages=[{\"role\": \"system\", \"content\": system_prompt}, \n",
    "                      {\"role\": \"user\", \"content\": user_prompt}],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        response_content = response.choices[0].message.content\n",
    "        if isinstance(response_content, str):\n",
    "            response_content = json.loads(response_content)\n",
    "        \n",
    "        response_content['execution_time'] = execution_time\n",
    "        \n",
    "        return response_content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during chat: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2f77bf-ef4a-496f-bc5d-fbe150fdea00",
   "metadata": {},
   "source": [
    "## GPT 4 (version 1106 with json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37c1f52d-0f0f-416c-bdcd-474ab6b3d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt4_v1106(key, system_prompt, user_prompt):\n",
    "    from openai import AzureOpenAI\n",
    "    import time\n",
    "\n",
    "    try:\n",
    "        client = AzureOpenAI(azure_endpoint=AZURE_OPENAI_ENDPOINT, \n",
    "                             api_key=key, \n",
    "                             api_version=AZURE_OPENAI_VERSION)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_MODEL_4_1106,\n",
    "            messages=[{\"role\": \"system\", \"content\": system_prompt}, \n",
    "                      {\"role\": \"user\", \"content\": user_prompt}],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        response_content = response.choices[0].message.content\n",
    "        if isinstance(response_content, str):\n",
    "            response_content = json.loads(response_content)\n",
    "        \n",
    "        response_content['execution_time'] = execution_time\n",
    "        \n",
    "        return response_content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during chat: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c959283f-d8bd-464f-bd0a-4000b1ef155d",
   "metadata": {},
   "source": [
    "## Models via Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91287f39-6a44-4eff-98ae-27590af7d4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama(model, system_prompt, user_prompt):\n",
    "    import ollama\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = ollama.chat(model=model, \n",
    "                               messages=[{'role': 'assistant', 'content': system_prompt},\n",
    "                                         {'role': 'user', 'content': user_prompt}],\n",
    "                               format='json'\n",
    "                         )\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        response_content = json.loads(response['message']['content'])\n",
    "        response_content['execution_time'] = execution_time\n",
    "        \n",
    "        return response_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error during chat: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e9e0a76-68ce-4f3e-a2a9-15bd1e6602f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = 'llama3:latest'\n",
    "# system_prompt =\"You are a helpful agent. Your answers must be a json with key 'output'\"\n",
    "# user_prompt = \"hi!\"\n",
    "# ollama(model, system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3236838-25c0-4bf4-99bc-11953babb8d8",
   "metadata": {},
   "source": [
    "## Function to validate the output of a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e50a3dcf-e321-4b0a-a721-aff7302589f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_output_validator(response_content, required_keys=None):\n",
    "    if response_content is None:\n",
    "        raise ValueError(\"Response content is None\")\n",
    "    \n",
    "    try:\n",
    "        if isinstance(response_content, str):\n",
    "            response_json = json.loads(response_content)\n",
    "        else:\n",
    "            response_json = response_content\n",
    "        \n",
    "        if required_keys and not all(key in response_json for key in required_keys):\n",
    "            raise ValueError(f\"Missing required keys: {required_keys}\")\n",
    "        \n",
    "        return response_json\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Invalid response content: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2468c-632e-44a5-a07a-fa609c1e8caa",
   "metadata": {},
   "source": [
    "# Merging all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52e3ac17-043c-4ed0-88ec-c05b729fe0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(model, system_prompt, user_prompt, key=None, max_attempts=1, required_keys=None):\n",
    "    \n",
    "    attempts = 0\n",
    "\n",
    "    while attempts < max_attempts:\n",
    "        if model == 'gpt35turbo_v1106':    \n",
    "            response_content = gpt35turbo_v1106(key, system_prompt, user_prompt)\n",
    "            \n",
    "        elif model == 'gpt4_v1106':    \n",
    "            response_content = gpt4_v1106(key, system_prompt, user_prompt)\n",
    "\n",
    "        elif model in ['mixtral:latest','phi3:medium','llama3:latest','llama2:latest']:\n",
    "            response_content = ollama(model, system_prompt, user_prompt)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model specified.\")\n",
    "        \n",
    "        try:\n",
    "            # Validate the output\n",
    "            valid_output = llm_output_validator(response_content, required_keys)\n",
    "            return valid_output\n",
    "        \n",
    "        except ValueError as e:\n",
    "            print(f\"Attempt {attempts + 1} failed: {e}\")\n",
    "            attempts += 1\n",
    "\n",
    "    print(f\"Output could not be validated within {max_attempts} attempts\")\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641a226c-5214-4a18-ac8d-6726af45ec4c",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17501684-9e60-44e3-a4a9-dd16fcb4d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt =\"You are a helpful assistant. Your answers must be a json with key 'output'\"\n",
    "# user_prompt = \"hi!\"\n",
    "\n",
    "# print('gpt35turbo_v1106:\\n',llm('gpt35turbo_v1106', system_prompt, user_prompt, key=AZURE_OPENAI_KEY,max_attempts=3,required_keys=['output']))\n",
    "# print()\n",
    "# print('gpt4_v1106:\\n',llm('gpt4_v1106', system_prompt, user_prompt, key=AZURE_OPENAI_KEY,max_attempts=3,required_keys=['output']))\n",
    "# print()\n",
    "# print('phi3:medium:\\n',llm('phi3:medium', system_prompt, user_prompt,max_attempts=3,required_keys=['output']))\n",
    "# print()\n",
    "# print('llama3:latest:\\n',llm('llama3:latest', system_prompt, user_prompt,max_attempts=3,required_keys=['output']))\n",
    "# print()\n",
    "# print('llama2:latest:\\n',llm('llama2:latest', system_prompt, user_prompt,max_attempts=3,required_keys=['output']))\n",
    "# print()\n",
    "# print('mixtral:latest:\\n',llm('mixtral:latest', system_prompt, user_prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
